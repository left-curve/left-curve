groups:
- name: instance-alerts
  rules:
  - alert: MetricsUnavailable
    expr: up{environment!="preview"} == 0
    for: 10m
    labels:
      severity: critical
    annotations:
      summary: "Prometheus metrics {{ $labels.instance }} down"
      description: "{{ $labels.instance }} of {{ $labels.job }} has been down for more than 10 minutes."

  - alert: HighCpuLoadWarning
    expr: (1 - avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m]))) * 100 > 80
    for: 15m
    labels:
      severity: warning
    annotations:
      summary: "High CPU load on {{ $labels.instance }} (warning)"
      description: "CPU usage > 80% for 15m. Investigate sustained load."

  - alert: HighCpuLoadCritical
    expr: (1 - avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m]))) * 100 > 90
    for: 15m
    labels:
      severity: critical
    annotations:
      summary: "High CPU load on {{ $labels.instance }} (critical)"
      description: "CPU usage > 90% for 15m. Likely saturation."

  - alert: HighMemoryUsageWarning
    expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "High memory usage on {{ $labels.instance }} (warning)"
      description: "Memory usage > 85% for 10m. Check processes and caches."

  - alert: HighMemoryUsageCritical
    expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 95
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "High memory usage on {{ $labels.instance }} (critical)"
      description: "Memory usage > 95% for 5m. Risk of OOM."

  - alert: DiskSpaceLowWarning
    expr: ((1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|overlay|squashfs",device!~"ram.*|loop.*",mountpoint!~"/run($|/)|/var/lib/docker($|/)|/var/lib/containers($|/)"} / node_filesystem_size_bytes{fstype!~"tmpfs|overlay|squashfs",device!~"ram.*|loop.*",mountpoint!~"/run($|/)|/var/lib/docker($|/)|/var/lib/containers($|/)"})) * 100) > 80 and on(instance,device,mountpoint) node_filesystem_readonly == 0
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Disk space low on {{ $labels.instance }}: {{ $labels.mountpoint }} (warning)"
      description: "Filesystem {{ $labels.device }} at {{ $labels.mountpoint }} > 80% for 10m."

  - alert: DiskSpaceLowCritical
    expr: ((1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|overlay|squashfs",device!~"ram.*|loop.*",mountpoint!~"/run($|/)|/var/lib/docker($|/)|/var/lib/containers($|/)"} / node_filesystem_size_bytes{fstype!~"tmpfs|overlay|squashfs",device!~"ram.*|loop.*",mountpoint!~"/run($|/)|/var/lib/docker($|/)|/var/lib/containers($|/)"})) * 100) > 90 and on(instance,device,mountpoint) node_filesystem_readonly == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Disk space low on {{ $labels.instance }}: {{ $labels.mountpoint }} (critical)"
      description: "Filesystem {{ $labels.device }} at {{ $labels.mountpoint }} > 90% for 5m."

- name: application-alerts
  rules:
  # This is now done through uptimekuma -> Discord
  # - alert: DangoServiceDown
  #   expr: up{job="dango", environment!="preview"} == 0
  #   for: 1m
  #   labels:
  #     severity: critical
  #   annotations:
  #     summary: "Dango service is down on {{ $labels.instance }} ({{ $labels.dango_network }})"
  #     description: "Dango service has been down for more than 1 minute."

  # This is now done through uptimekuma -> Discord
  # - alert: CometBFTServiceDown
  #   expr: up{job="cometbft", environment!="preview"} == 0
  #   for: 1m
  #   labels:
  #     severity: critical
  #   annotations:
  #     summary: "CometBFT service is down on {{ $labels.instance }} ({{ $labels.dango_network }})"
  #     description: "CometBFT service has been down for more than 1 minute."

  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High error rate detected"
      description: "Error rate is above 10% for more than 5 minutes."
